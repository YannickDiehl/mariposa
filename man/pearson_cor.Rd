% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pearson_cor.R
\name{pearson_cor}
\alias{pearson_cor}
\title{Measure How Strongly Variables Are Related}
\usage{
pearson_cor(data, ..., weights = NULL, conf.level = 0.95, na.rm = "pairwise")
}
\arguments{
\item{data}{Your survey data (a data frame or tibble)}

\item{...}{The numeric variables you want to correlate. List two for a single
correlation or more for a correlation matrix.}

\item{weights}{Optional survey weights for population-representative results}

\item{conf.level}{Confidence level for intervals (Default: 0.95 = 95\%)}

\item{na.rm}{How to handle missing values:
\itemize{
\item \code{"pairwise"} (default): Use all available data for each pair
\item \code{"listwise"}: Only use complete cases across all variables
}}
}
\value{
Correlation results showing relationships between variables, including:
\itemize{
\item Correlation coefficient (r): Strength and direction of relationship
\item P-value: Whether the relationship is statistically significant
\item Confidence interval: Range of plausible correlation values
\item Sample size: Number of observations used
}
}
\description{
\code{pearson_cor()} shows you how strongly numeric variables are related to each
other. For example, is age related to income? Does satisfaction increase with
experience? This helps you understand patterns in your data.

The correlation tells you:
\itemize{
\item \strong{Direction}: Positive (both increase together) or negative (one increases as other decreases)
\item \strong{Strength}: How closely the variables move together (from 0 = no relationship to 1 = perfect relationship)
\item \strong{Significance}: Whether the relationship is real or could be due to chance
}
}
\details{
\subsection{Understanding Correlation Values}{

\strong{Correlation coefficient (r)} ranges from -1 to +1:
\itemize{
\item \strong{+1}: Perfect positive relationship (as one goes up, the other always goes up)
\item \strong{0}: No linear relationship
\item \strong{-1}: Perfect negative relationship (as one goes up, the other always goes down)
}

\strong{Interpreting strength} (absolute value of r):
\itemize{
\item 0.00 - 0.10: Negligible relationship
\item 0.10 - 0.30: Weak relationship
\item 0.30 - 0.50: Moderate relationship
\item 0.50 - 0.70: Strong relationship
\item 0.70 - 0.90: Very strong relationship
\item 0.90 - 1.00: Extremely strong relationship
}

\strong{P-value interpretation}:
\itemize{
\item p < 0.001: Very strong evidence of a relationship
\item p < 0.01: Strong evidence of a relationship
\item p < 0.05: Moderate evidence of a relationship
\item p ≥ 0.05: No significant relationship found
}
}

\subsection{When to Use Pearson Correlation}{

Use this when:
\itemize{
\item Both variables are numeric and continuous
\item You expect a linear relationship
\item Data is roughly normally distributed
\item You want to measure strength of linear association
}

Don't use when:
\itemize{
\item Data has extreme outliers (consider Spearman instead)
\item Relationship is curved/non-linear
\item Variables are categorical (use chi-squared test)
\item You need to establish causation (correlation ≠ causation!)
}
}

\subsection{Reading the Results}{

A correlation of 0.65 with p < 0.001 means:
\itemize{
\item Strong positive relationship (r = 0.65)
\item As one variable increases, the other tends to increase
\item Very unlikely to be due to chance (p < 0.001)
\item About 42\% of variation is shared (r² = 0.65² = 0.42)
}
}

\subsection{Tips for Success}{
\itemize{
\item Always plot your data first to check for non-linear patterns
\item Consider both statistical significance (p-value) and practical importance (r value)
\item Remember: correlation does not imply causation
\item Check for outliers that might inflate or deflate correlations
\item Use Spearman correlation for ordinal data or non-normal distributions
}
}
}
\examples{
# Load required packages and data
library(dplyr)
data(survey_data)

# Basic correlation between two variables
survey_data \%>\% 
  pearson_cor(age, income)

# Correlation matrix for multiple variables
survey_data \%>\% 
  pearson_cor(age, income, life_satisfaction)

# Weighted correlations
survey_data \%>\% 
  pearson_cor(age, income, weights = sampling_weight)

# Grouped correlations
survey_data \%>\% 
  group_by(region) \%>\% 
  pearson_cor(age, income, life_satisfaction)

# Using tidyselect helpers
survey_data \%>\% 
  pearson_cor(where(is.numeric), weights = sampling_weight)

# Listwise deletion for missing data
survey_data \%>\% 
  pearson_cor(age, income, na.rm = "listwise")

# Store results for further analysis
result <- survey_data \%>\% 
  pearson_cor(age, income, life_satisfaction, weights = sampling_weight)
print(result)

}
\references{
Cohen, J. (1988). Statistical Power Analysis for the Behavioral Sciences (2nd ed.).
Lawrence Erlbaum Associates.

Fisher, R.A. (1915). Frequency distribution of the values of the correlation coefficient
in samples from an indefinitely large population. Biometrika, 10(4), 507-521.
}
\seealso{
\code{\link[stats]{cor}} for the base R correlation function
\code{\link[stats]{cor.test}} for correlation significance testing
}
