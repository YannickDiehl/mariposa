---
title: "Comparing Groups and Testing Hypotheses"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Comparing Groups and Testing Hypotheses}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(mariposa)
library(dplyr)
data(survey_data)
```

## When to Use These Tests

Statistical tests help you determine whether differences between groups are real or just due to random chance. Here's when to use each test:

- **t-test**: Compare means between two groups
- **ANOVA**: Compare means across three or more groups
- **Chi-square**: Test relationships between categorical variables
- **Mann-Whitney**: Non-parametric alternative when data isn't normal

## t-Tests

### Independent Samples t-Test

Compare average life satisfaction between genders:

```{r}
survey_data %>%
  t_test(life_satisfaction, group = gender)
```

With survey weights for representative results:

```{r}
survey_data %>%
  t_test(life_satisfaction, group = gender, weights = sampling_weight)
```

### Multiple Variables at Once

Test several outcomes simultaneously:

```{r}
# Test all trust variables between genders
survey_data %>%
  t_test(trust_government, trust_science, trust_media,
         group = gender, weights = sampling_weight)
```

### One-Sample t-Test

Test whether the population mean differs from a specific value:

```{r}
# Is average life satisfaction different from 3 (neutral)?
survey_data %>%
  t_test(life_satisfaction, mu = 3, weights = sampling_weight)
```

### Grouped Analysis

Run separate tests for each region:

```{r}
survey_data %>%
  group_by(region) %>%
  t_test(income, group = gender, weights = sampling_weight)
```

## One-Way ANOVA

### Basic ANOVA

Compare means across multiple groups (3+ categories):

```{r}
# Compare life satisfaction across education levels
survey_data %>%
  oneway_anova(life_satisfaction, group = education)
```

### With Survey Weights

```{r}
survey_data %>%
  oneway_anova(life_satisfaction, group = education,
               weights = sampling_weight)
```

### Post-Hoc Tests

When ANOVA is significant, use post-hoc tests to see which groups differ:

```{r}
# First run ANOVA and save result
anova_result <- survey_data %>%
  oneway_anova(life_satisfaction, group = education,
               weights = sampling_weight)

# Then run Tukey test on the result
tukey_test(anova_result)

# Or Scheffe test (more conservative)
scheffe_test(anova_result)
```

### Testing Assumptions

Check if variances are equal across groups:

```{r}
levene_test(anova_result)
```

## Chi-Square Test

### Basic Chi-Square

Test if two categorical variables are related:

```{r}
# Is education related to employment status?
survey_data %>%
  chi_square(education, employment)
```

### With Survey Weights

```{r}
survey_data %>%
  chi_square(education, employment, weights = sampling_weight)
```

### Multiple Tests

Test several relationships separately:

```{r}
# Test association between employment and education
survey_data %>%
  chi_square(employment, education, weights = sampling_weight)

# Test association between employment and gender
survey_data %>%
  chi_square(employment, gender, weights = sampling_weight)

# Test association between employment and region
survey_data %>%
  chi_square(employment, region, weights = sampling_weight)
```

## Mann-Whitney Test

### When to Use

Use when:
- Data is not normally distributed
- Sample sizes are small
- Data is ordinal (ranked)

### Basic Usage

```{r}
# Compare political orientation between regions
survey_data %>%
  mann_whitney(political_orientation, group = region)
```

### With Survey Weights

```{r}
survey_data %>%
  mann_whitney(political_orientation, group = region,
               weights = sampling_weight)
```

## Interpreting Results

### Understanding p-values

- **p < 0.05**: Statistically significant difference
- **p ≥ 0.05**: No significant difference detected
- Remember: "not significant" ≠ "no difference"

### Effect Sizes Matter

Statistical significance doesn't always mean practical importance:

```{r}
# ANOVA provides eta-squared effect size
result <- survey_data %>%
  oneway_anova(income, group = education, weights = sampling_weight)
print(result)
```

Effect size guidelines:
- Small: η² = 0.01
- Medium: η² = 0.06
- Large: η² = 0.14

### Sample Size Effects

Large samples can make tiny differences "significant":
- Always check effect sizes
- Consider practical significance
- Look at confidence intervals

## Complete Example

Here's a typical hypothesis testing workflow:

```{r}
# 1. Descriptive statistics first
cat("=== Descriptive Summary ===\n")
survey_data %>%
  group_by(education) %>%
  describe(life_satisfaction, weights = sampling_weight)

# 2. Test for overall difference
cat("\n=== ANOVA Test ===\n")
anova_result <- survey_data %>%
  oneway_anova(life_satisfaction, group = education,
               weights = sampling_weight)
print(anova_result)

# 3. If significant, see which groups differ
if (!is.null(anova_result) && "p_value" %in% names(anova_result) && !is.na(anova_result$p_value[1]) && anova_result$p_value[1] < 0.05) {
  cat("\n=== Post-hoc Comparisons ===\n")
  tukey_test(anova_result)
}

# 4. Check assumptions
cat("\n=== Assumption Check ===\n")
levene_test(anova_result)
```

## Best Practices

### 1. Check Assumptions First

Before t-test or ANOVA:
```{r}
# Visual check for normality
# (In practice, create histograms or Q-Q plots)
survey_data %>%
  group_by(gender) %>%
  describe(life_satisfaction, show = c("skew", "kurtosis"))
```

### 2. Use Appropriate Tests

- **Normal data + equal variances**: t-test/ANOVA
- **Non-normal or ordinal**: Mann-Whitney
- **Categorical**: Chi-square

### 3. Report Completely

Always report:
- Test statistic
- Degrees of freedom
- p-value
- Effect size
- Confidence intervals

### 4. Multiple Comparisons

When running many tests, consider:
- Bonferroni correction
- False Discovery Rate (FDR)
- Pre-registration of hypotheses

## Common Mistakes to Avoid

1. **Ignoring assumptions**: Check normality and variance
2. **p-hacking**: Don't test everything hoping for significance
3. **Ignoring weights**: Unweighted tests can be misleading
4. **Over-interpreting p-values**: p = 0.049 vs p = 0.051 is not meaningfully different
5. **Forgetting effect sizes**: Significance ≠ importance

## Next Steps

After finding significant differences:
- Explore with `crosstab()` for categorical relationships
- Use `pearson_cor()` for continuous associations
- Consider regression models for controlling confounds
- Validate findings with different subgroups